<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><title>U-Net | Orion Blog</title><meta name="author" content="Orion"><meta name="description" content="Here's Orion"><meta name="keywords" content="技术博客,Orion,BUPT北京邮电大学,本科生,人工智能,AI算法工程师,机器学习,深度学习,计算机视觉,自然语言处理,NLP,CV,大模型"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0"><link rel="icon" href="https://testingcf.jsdelivr.net/gh/nanzhi84/blog-images/202409190134514.png"><link rel="preload" as="image" href="https://testingcf.jsdelivr.net/gh/nanzhi84/blog-images/202409181543316.png"><link rel="preconnect" href="https://s4.zstatic.net"><script src="https://s4.zstatic.net/ajax/libs/vue/3.3.7/vue.global.prod.min.js"></script><link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/font-awesome/6.4.2/css/all.min.css" media="only x" onload='this.onload=null,this.media="all"'><link rel="preconnect" href="https://fonts.googleapis.cn"><link rel="preconnect" href="https://fonts.gstatic.cn" crossorigin><link rel="stylesheet" href="https://fonts.googleapis.cn/css2?family=Fira+Code:wght@400;500;600;700&family=Lexend:wght@400;500;600;700;800;900&family=Noto+Sans+SC:wght@400;500;600;700;800;900&display=swap" media="only x" onload='this.onload=null,this.media="all"'><script>const mixins={}</script><script src="https://s4.zstatic.net/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script><script src="https://s4.zstatic.net/ajax/libs/highlightjs-line-numbers.js/2.8.0/highlightjs-line-numbers.min.js"></script><link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/highlight.js/11.9.0/styles/github.min.css"><script src="/js/lib/highlight.js"></script><script src="https://s4.zstatic.net/ajax/libs/KaTeX/0.16.9/katex.min.js"></script><script src="https://s4.zstatic.net/ajax/libs/KaTeX/0.16.9/contrib/auto-render.min.js"></script><link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/KaTeX/0.16.9/katex.min.css"><script src="/js/lib/math.js"></script><script src="/js/lib/preview.js"></script><script src="https://s4.zstatic.net/ajax/libs/waline/2.15.8/waline.min.js"></script><link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/waline/2.15.8/waline.min.css"><link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/waline/2.15.8/waline-meta.min.css"><script src="https://cdn.staticfile.org/animejs/3.2.1/anime.min.js"></script><link rel="stylesheet" href="/css/main.css"><link rel="preconnect" href="https://static-argvchs.netlify.app"><meta name="generator" content="Hexo 7.3.0"><style>mjx-container[jax=SVG]{direction:ltr}mjx-container[jax=SVG]>svg{overflow:visible}mjx-container[jax=SVG][display=true]{display:block;text-align:center;margin:1em 0}mjx-container[jax=SVG][justify=left]{text-align:left}mjx-container[jax=SVG][justify=right]{text-align:right}g[data-mml-node=merror]>g{fill:red;stroke:red}g[data-mml-node=merror]>rect[data-background]{fill:yellow;stroke:none}g[data-mml-node=mtable]>line[data-line]{stroke-width:70px;fill:none}g[data-mml-node=mtable]>rect[data-frame]{stroke-width:70px;fill:none}g[data-mml-node=mtable]>.mjx-dashed{stroke-dasharray:140}g[data-mml-node=mtable]>.mjx-dotted{stroke-linecap:round;stroke-dasharray:0,140}g[data-mml-node=mtable]>svg{overflow:visible}[jax=SVG] mjx-tool{display:inline-block;position:relative;width:0;height:0}[jax=SVG] mjx-tool>mjx-tip{position:absolute;top:0;left:0}mjx-tool>mjx-tip{display:inline-block;padding:.2em;border:1px solid #888;font-size:70%;background-color:#f8f8f8;color:#000;box-shadow:2px 2px 5px #aaa}g[data-mml-node=maction][data-toggle]{cursor:pointer}mjx-status{display:block;position:fixed;left:1em;bottom:1em;min-width:25%;padding:.2em .4em;border:1px solid #888;font-size:90%;background-color:#f8f8f8;color:#000}foreignObject[data-mjx-xml]{font-family:initial;line-height:normal;overflow:visible}.MathJax path{stroke-width:3}mjx-container[display=true]{overflow:auto hidden}mjx-container[display=true]+br{display:none}</style></head><body><div id="layout"><transition name="fade"><div id="loading" v-show="loading"><div id="loading-circle"><h2>LOADING</h2><p>加载过慢请开启缓存 浏览器默认开启</p><img src="https://testingcf.jsdelivr.net/gh/nanzhi84/blog-images/202409201808995.gif"></div></div></transition><div id="menu" :class="{ hidden: hiddenMenu, 'menu-color': menuColor}"><nav id="desktop-menu"><a class="title" href="/"><span>ORION BLOG</span></a><a href="/"><i class="fa-solid fa-house fa-fw"></i> <span>&ensp;Home</span></a><a href="/about"><i class="fa-solid fa-id-card fa-fw"></i> <span>&ensp;About</span></a><a href="/archives"><i class="fa-solid fa-box-archive fa-fw"></i> <span>&ensp;Archives</span></a><a href="/categories"><i class="fa-solid fa-bookmark fa-fw"></i> <span>&ensp;Categories</span></a><a href="/tags"><i class="fa-solid fa-tags fa-fw"></i> <span>&ensp;Tags</span></a></nav><nav id="mobile-menu"><div class="title" @click="showMenuItems = !showMenuItems"><i class="fa-solid fa-bars fa-fw"></i> <span>&emsp;ORION BLOG</span></div><transition name="slide"><div class="items" v-show="showMenuItems"><a href="/"><div class="item"><div style="min-width:20px;max-width:50px;width:10%"><i class="fa-solid fa-house fa-fw"></i></div><div style="min-width:100px;max-width:150%;width:20%">Home</div></div></a><a href="/about"><div class="item"><div style="min-width:20px;max-width:50px;width:10%"><i class="fa-solid fa-id-card fa-fw"></i></div><div style="min-width:100px;max-width:150%;width:20%">About</div></div></a><a href="/archives"><div class="item"><div style="min-width:20px;max-width:50px;width:10%"><i class="fa-solid fa-box-archive fa-fw"></i></div><div style="min-width:100px;max-width:150%;width:20%">Archives</div></div></a><a href="/categories"><div class="item"><div style="min-width:20px;max-width:50px;width:10%"><i class="fa-solid fa-bookmark fa-fw"></i></div><div style="min-width:100px;max-width:150%;width:20%">Categories</div></div></a><a href="/tags"><div class="item"><div style="min-width:20px;max-width:50px;width:10%"><i class="fa-solid fa-tags fa-fw"></i></div><div style="min-width:100px;max-width:150%;width:20%">Tags</div></div></a></div></transition></nav></div><transition name="fade"><div id="menu-curtain" @click="showMenuItems = !showMenuItems" v-show="showMenuItems"></div></transition><div id="main" :class="loading ? 'into-enter-from': 'into-enter-active'"><div class="article"><div><h1>U-Net</h1></div><div class="info"><span class="date"><span class="icon"><i class="fa-solid fa-calendar fa-fw"></i></span> 2024/9/16</span><span class="category"><a href="/categories/%E8%AF%BB%E8%AE%BA%E6%96%87%E7%B3%BB%E5%88%97/"><span class="icon"><i class="fa-solid fa-bookmark fa-fw"></i></span> 读论文系列</a></span><span class="tags"><span class="icon"><i class="fa-solid fa-tags fa-fw"></i></span> <span class="tag"><a href="/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/" style="color:#00bcd4">图像分割</a></span> <span class="tag"><a href="/tags/U-Net/" style="color:#ffa2c4">U-Net</a></span> <span class="tag"><a href="/tags/Encoder-Decoder/" style="color:#ff7d73">Encoder-Decoder</a></span></span></div><div class="content" v-pre><p><img src="https://testingcf.jsdelivr.net/gh/nanzhi84/blog-images/202409180101311.png" alt="image-20240916141619732"></p><p>论文发布日期：2015-05-18</p><p>发布会议/期刊：MICCAI 2015（CCF-B）</p><span id="more"></span><p>阅读时，请注意以下观点发布的时间是在2015年</p><p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD</p><h1><span id="abstract">Abstract</span><a href="#abstract" class="header-anchor"></a></h1><p>=======<br><br></p><h1><span id="abstract">Abstract</span><a href="#abstract" class="header-anchor"></a></h1><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><p>7cb2d73ccef54ba0eab1b435972b62a26ba4604a<br>- 人们普遍认为成功训练深度网络需要上千个带标号的样本<br>- 作者提出了一种网络和训练策略，依赖于数据增强的强力应用来更有效地使用标号样本<br>- 该架构由一个捕捉上下文的收缩路径和实现精确定位的对称扩展路径组成<br>- 该网络可以从很少图像中进行端到端的训练，并且取得了SOTA</p></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote><p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD</p><h1><span id="introduction">Introduction</span><a href="#introduction" class="header-anchor"></a></h1><p>=======<br><br></p><h1><span id="introduction">Introduction</span><a href="#introduction" class="header-anchor"></a></h1><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><p>7cb2d73ccef54ba0eab1b435972b62a26ba4604a<br><strong>目标</strong></p></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote><p>如何在仅有少量样本时，能充分利用图片上下文并保持像素定位精度，来对生物医学图像进行精准分割</p><p><strong>动机</strong></p><ol type="1"><li>过去卷积神经网络通常用于为每张图输出分类标签，然而在许多视觉任务例如生物医学图像处理中，所需的输出应该包括定位，即应该为每个像素分配一个类标签</li><li>在生物医学任务中，数以千计的训练图像通常是遥不可及的。过去Ciresan等人的工作设置滑动窗口来训练网络，通过提供该像素周围的局部区域(patch)作为输入来预测每个像素的类标签。这种方法有两个缺点：一是非常慢并且有大量冗余。二是定位精度和上下文选择之间需要权衡，因为较大的patch需要更大的最大池化，会降低定位精度，而较小的patch仅仅看到了很少的上下文</li><li>最近有方法提出考虑多层特征分类器输出，可以实现同时良好的定位和使用上下文信息</li></ol><p><strong>解决方法</strong></p><ol type="1"><li>U-Net包含一个收缩路径（用于捕捉上下文信息）和一个对称的扩张路径（用于精确定位），并且还有两个路径特征图的<strong>skip connection</strong>，可以增强前后文信息传递</li><li>生物医学图像标注较少，U-Net通过弹性形变进行数据增强，更好地利用了标注样本</li><li>Overlap-tile strategy用于任意大图像的无缝分割，即滑动窗口重叠。因为特征图大小不匹配，还用镜像推断用周边镜像数据预测输出</li></ol><p><img src="https://testingcf.jsdelivr.net/gh/nanzhi84/blog-images/202409180101312.png" alt="image-20240916155916184"></p><p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD</p><h1><span id="network-architecture">Network Architecture</span><a href="#network-architecture" class="header-anchor"></a></h1><p>=======<br><br></p><h1><span id="network-architecture">Network Architecture</span><a href="#network-architecture" class="header-anchor"></a></h1><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><p>7cb2d73ccef54ba0eab1b435972b62a26ba4604a<br><img src="https://testingcf.jsdelivr.net/gh/nanzhi84/blog-images/202409180101313.png" alt="image-20240916145638148"></p></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote><p>要点解析</p><ol type="1"><li>卷积层conv 3*3没有padding会减少长宽，后跟ReLU</li><li>下采样后经过的两层卷积会将channel翻倍</li><li>上采样(up-conv)利用转置卷积实现，每次上采样后channel减半</li><li>将之前对应的channel的特征图进行中心裁剪后与上采样后的concat，再通过两层卷积减半channel</li><li>最后一层通过conv 1*1无ReLU，卷积核的个数与需要分类的个数有关，原文只有背景和前景两类</li></ol><p>主流改进</p><ol type="1"><li>将conv 3*3层加上padding，不改变图像大小</li><li>卷积和ReLU中会加上Batch Normalization层，即BN层</li></ol><p>这样最后会生成与原图一样大小的特征图</p><p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD</p><h1><span id="training">Training</span><a href="#training" class="header-anchor"></a></h1><p>=======<br><br></p><h1><span id="training">Training</span><a href="#training" class="header-anchor"></a></h1><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><p>7cb2d73ccef54ba0eab1b435972b62a26ba4604a<br>1. 训练策略</p></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote><pre><code>- batch size = 1
- 动量 $\beta = 0.99$ 2. 加权损失函数

$$
\begin{align}
E &amp;= \sum_{\mathbf{x} \in \Omega} w(\mathbf{x}) \log(p_{\ell(\mathbf{x})}(\mathbf{x})) \\
w(\mathbf{x}) &amp;= w_c(\mathbf{x}) + w_0 \cdot \exp\left(-\frac{(d_1(\mathbf{x}) + d_2(\mathbf{x}))^2}{2\sigma^2}\right)
\end{align}
$$

损失函数加权意味着细胞边界的细小分割会被给大权重，而多背景的部分会被赋极小权重

![image-20240916160847510](https://testingcf.jsdelivr.net/gh/nanzhi84/blog-images/202409180101314.png)</code></pre><ol start="3" type="1"><li>医学图像数据增强(略)</li></ol><p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD</p><h1><span id="experiments">Experiments</span><a href="#experiments" class="header-anchor"></a></h1><p>=======<br><br></p><h1><span id="experiments">Experiments</span><a href="#experiments" class="header-anchor"></a></h1><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><p>7cb2d73ccef54ba0eab1b435972b62a26ba4604a<br><img src="https://testingcf.jsdelivr.net/gh/nanzhi84/blog-images/202409180101315.png" alt="image-20240916160950897"></p></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote><p><img src="https://testingcf.jsdelivr.net/gh/nanzhi84/blog-images/202409180101316.png" alt="image-20240916160959499"></p><p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD</p><h1><span id="conclusion">Conclusion</span><a href="#conclusion" class="header-anchor"></a></h1><p>U-Net再不同的生物医学分割应用中实现了好性能，只需要很少带标注的图像就可以在GPU中实现极快的训练。</p><p>=======<br><br></p><h1><span id="conclusion">Conclusion</span><a href="#conclusion" class="header-anchor"></a></h1><p>U-Net再不同的生物医学分割应用中实现了好性能，只需要很少带标注的图像就可以在GPU中实现极快的训练。<br>&gt;&gt;&gt;&gt;&gt;&gt;&gt; 7cb2d73ccef54ba0eab1b435972b62a26ba4604a</p></div><div id="comment"><div id="waline-container"></div></div></div><footer id="footer"><div id="footer-wrap"><div>&copy; 2024 - 2024 Orion Blog<span id="footer-icon"><i class="fa-solid fa-font-awesome fa-fw"></i></span> &commat;Orion</div><div>Based on the <a target="_blank" rel="noopener" href="https://hexo.io">Hexo Engine</a> &amp; <a target="_blank" rel="noopener" href="https://github.com/theme-particlex/hexo-theme-particlex">ParticleX Theme</a></div></div></footer></div><transition name="fade"><div id="preview" ref="preview" v-show="previewShow"><img id="preview-content" ref="previewContent"></div></transition></div><script src="/js/main.js"></script><script>Waline.init({el:"#waline-container",serverURL:"https://www.orionverse-comment.blog/",commentCount:!0,pageview:!0,emoji:"https://unpkg.com/@waline/emojis@1.0.1/weibo,https://unpkg.com/@waline/emojis@1.0.1/alus,https://unpkg.com/@waline/emojis@1.0.1/bilibili,https://unpkg.com/@waline/emojis@1.0.1/qq,https://unpkg.com/@waline/emojis@1.0.1/tieba,https://unpkg.com/@waline/emojis@1.0.1/tw-emoji".split(","),meta:"nick,mail,link".split(","),requiredMeta:"nick".split(","),lang:"zh-CN",wordLimit:0,pageSize:"10",login:"enable"})</script></body></html>