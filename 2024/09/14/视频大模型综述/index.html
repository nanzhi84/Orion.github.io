<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><title>Video Understanding with LLMs | Orion Blog</title><meta name="author" content="Orion"><meta name="description" content="Here's Orion"><meta name="keywords" content="技术博客,Orion,BUPT北京邮电大学,本科生,人工智能,AI算法工程师,机器学习,深度学习,计算机视觉,自然语言处理,NLP,CV,大模型"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0"><link rel="icon" href="https://testingcf.jsdelivr.net/gh/nanzhi84/blog-images/202409190134514.png"><link rel="preload" as="image" href="https://testingcf.jsdelivr.net/gh/nanzhi84/blog-images/202409181543316.png"><link rel="preconnect" href="https://s4.zstatic.net"><script src="https://s4.zstatic.net/ajax/libs/vue/3.3.7/vue.global.prod.min.js"></script><link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/font-awesome/6.4.2/css/all.min.css" media="only x" onload='this.onload=null,this.media="all"'><link rel="preconnect" href="https://fonts.googleapis.cn"><link rel="preconnect" href="https://fonts.gstatic.cn" crossorigin><link rel="stylesheet" href="https://fonts.googleapis.cn/css2?family=Fira+Code:wght@400;500;600;700&family=Lexend:wght@400;500;600;700;800;900&family=Noto+Sans+SC:wght@400;500;600;700;800;900&display=swap" media="only x" onload='this.onload=null,this.media="all"'><script>const mixins={}</script><script src="https://s4.zstatic.net/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script><script src="https://s4.zstatic.net/ajax/libs/highlightjs-line-numbers.js/2.8.0/highlightjs-line-numbers.min.js"></script><link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/highlight.js/11.9.0/styles/github.min.css"><script src="/js/lib/highlight.js"></script><script src="https://s4.zstatic.net/ajax/libs/KaTeX/0.16.9/katex.min.js"></script><script src="https://s4.zstatic.net/ajax/libs/KaTeX/0.16.9/contrib/auto-render.min.js"></script><link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/KaTeX/0.16.9/katex.min.css"><script src="/js/lib/math.js"></script><script src="/js/lib/preview.js"></script><script src="https://s4.zstatic.net/ajax/libs/waline/2.15.8/waline.min.js"></script><link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/waline/2.15.8/waline.min.css"><link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/waline/2.15.8/waline-meta.min.css"><script src="https://cdn.staticfile.org/animejs/3.2.1/anime.min.js"></script><link rel="stylesheet" href="/css/main.css"><link rel="preconnect" href="https://static-argvchs.netlify.app"><meta name="generator" content="Hexo 7.3.0"><style>mjx-container[jax=SVG]{direction:ltr}mjx-container[jax=SVG]>svg{overflow:visible}mjx-container[jax=SVG][display=true]{display:block;text-align:center;margin:1em 0}mjx-container[jax=SVG][justify=left]{text-align:left}mjx-container[jax=SVG][justify=right]{text-align:right}g[data-mml-node=merror]>g{fill:red;stroke:red}g[data-mml-node=merror]>rect[data-background]{fill:yellow;stroke:none}g[data-mml-node=mtable]>line[data-line]{stroke-width:70px;fill:none}g[data-mml-node=mtable]>rect[data-frame]{stroke-width:70px;fill:none}g[data-mml-node=mtable]>.mjx-dashed{stroke-dasharray:140}g[data-mml-node=mtable]>.mjx-dotted{stroke-linecap:round;stroke-dasharray:0,140}g[data-mml-node=mtable]>svg{overflow:visible}[jax=SVG] mjx-tool{display:inline-block;position:relative;width:0;height:0}[jax=SVG] mjx-tool>mjx-tip{position:absolute;top:0;left:0}mjx-tool>mjx-tip{display:inline-block;padding:.2em;border:1px solid #888;font-size:70%;background-color:#f8f8f8;color:#000;box-shadow:2px 2px 5px #aaa}g[data-mml-node=maction][data-toggle]{cursor:pointer}mjx-status{display:block;position:fixed;left:1em;bottom:1em;min-width:25%;padding:.2em .4em;border:1px solid #888;font-size:90%;background-color:#f8f8f8;color:#000}foreignObject[data-mjx-xml]{font-family:initial;line-height:normal;overflow:visible}.MathJax path{stroke-width:3}mjx-container[display=true]{overflow:auto hidden}mjx-container[display=true]+br{display:none}</style></head><body><div id="layout"><transition name="fade"><div id="loading" v-show="loading"><div id="loading-circle"><h2>LOADING</h2><p>加载过慢请开启缓存 浏览器默认开启</p><img src="https://testingcf.jsdelivr.net/gh/nanzhi84/blog-images/202409201808995.gif"></div></div></transition><div id="menu" :class="{ hidden: hiddenMenu, 'menu-color': menuColor}"><nav id="desktop-menu"><a class="title" href="/"><span>ORION BLOG</span></a><a href="/"><i class="fa-solid fa-house fa-fw"></i> <span>&ensp;Home</span></a><a href="/about"><i class="fa-solid fa-id-card fa-fw"></i> <span>&ensp;About</span></a><a href="/archives"><i class="fa-solid fa-box-archive fa-fw"></i> <span>&ensp;Archives</span></a><a href="/categories"><i class="fa-solid fa-bookmark fa-fw"></i> <span>&ensp;Categories</span></a><a href="/tags"><i class="fa-solid fa-tags fa-fw"></i> <span>&ensp;Tags</span></a></nav><nav id="mobile-menu"><div class="title" @click="showMenuItems = !showMenuItems"><i class="fa-solid fa-bars fa-fw"></i> <span>&emsp;ORION BLOG</span></div><transition name="slide"><div class="items" v-show="showMenuItems"><a href="/"><div class="item"><div style="min-width:20px;max-width:50px;width:10%"><i class="fa-solid fa-house fa-fw"></i></div><div style="min-width:100px;max-width:150%;width:20%">Home</div></div></a><a href="/about"><div class="item"><div style="min-width:20px;max-width:50px;width:10%"><i class="fa-solid fa-id-card fa-fw"></i></div><div style="min-width:100px;max-width:150%;width:20%">About</div></div></a><a href="/archives"><div class="item"><div style="min-width:20px;max-width:50px;width:10%"><i class="fa-solid fa-box-archive fa-fw"></i></div><div style="min-width:100px;max-width:150%;width:20%">Archives</div></div></a><a href="/categories"><div class="item"><div style="min-width:20px;max-width:50px;width:10%"><i class="fa-solid fa-bookmark fa-fw"></i></div><div style="min-width:100px;max-width:150%;width:20%">Categories</div></div></a><a href="/tags"><div class="item"><div style="min-width:20px;max-width:50px;width:10%"><i class="fa-solid fa-tags fa-fw"></i></div><div style="min-width:100px;max-width:150%;width:20%">Tags</div></div></a></div></transition></nav></div><transition name="fade"><div id="menu-curtain" @click="showMenuItems = !showMenuItems" v-show="showMenuItems"></div></transition><div id="main" :class="loading ? 'into-enter-from': 'into-enter-active'"><div class="article"><div><h1>Video Understanding with LLMs</h1></div><div class="info"><span class="date"><span class="icon"><i class="fa-solid fa-calendar fa-fw"></i></span> 2024/9/14</span><span class="category"><a href="/categories/%E8%AF%BB%E8%AE%BA%E6%96%87%E7%B3%BB%E5%88%97/"><span class="icon"><i class="fa-solid fa-bookmark fa-fw"></i></span> 读论文系列</a></span><span class="tags"><span class="icon"><i class="fa-solid fa-tags fa-fw"></i></span> <span class="tag"><a href="/tags/%E8%A7%86%E9%A2%91%E5%A4%A7%E6%A8%A1%E5%9E%8B/" style="color:#00a596">视频大模型</a></span> <span class="tag"><a href="/tags/%E7%BB%BC%E8%BF%B0/" style="color:#03a9f4">综述</a></span></span></div><div class="content" v-pre><p><img src="https://testingcf.jsdelivr.net/gh/nanzhi84/blog-images/202409180058265.png" alt="image-20240914113409026"></p><p>论文发布日期:2024-07-24</p><span id="more"></span><h1 id="abstract">Abstract</h1><p>Vid-LLMs 的三个主类（按方法分）：</p><ul><li>Video Analyzer × LLM</li><li>Video Embedder × LLM</li><li>(Analyzer + Embedder) × LLM</li></ul><p>Vid-LLMs 的五个子类（按功能分）：</p><ul><li>LLM as Summarizer</li><li>LLM as Manager</li><li>LLM as Text Decoder</li><li>LLM as Regressor</li><li>LLM as Hidden Layer</li></ul><h1 id="introduction">Introduction</h1><p>A. 视频理解方法的发展历程</p><ol type="1"><li><p>传统方法：</p><ul><li>SIFT、SURF、HOG、背景扣除、光流、IDT（光流？）、HMM、SVM、决策树、随机森林、PCA</li></ul></li><li><p>早期神经网络视频模型：</p><ul><li>Deepvideo，引入深度 CNN，打不过手工特征方法，因为失去了运动特征</li><li>Two-stream networks，结合了 CNN 和光流来捕获运动信息，能与手工打平</li><li>为了处理长视频，LSTM、TSN、FV 编码、双线性编码、VLAD，显著提高了 UCF101 和 HMDB51 性能</li><li>3D 卷积出现打开了新局面，I3D-&gt;R3D、MFNet、STC 出现，显著提高性能，出现 K400 和 something-something 数据集评估更具挑战的性能</li><li>3D 卷积分解为 2D 卷积和 1D 卷积，S3D、ECO、P3D</li><li>LTC、T3D、Non-local、V4D 专注于长时间建模，CSN、SlowFast、X3D 倾向于获得高效率</li><li>ViT 引入又促进了一系列新的模型诞生，例如 TimeSformer、VidTr、ViViT、MViT</li></ul></li><li><p>自监督视频预训练模型：</p><ul><li><p>VideoBERT, 基于双向语言模型 BERT，相关任务用于自监督 video-text 数据。特征处理中，VideoBERT 利用了层次化的 k-means 对视频特征进行分词</p></li><li><p>遵循预训练微调范式，出现了许多针对视频理解的预训练模型，分为使用不同的架构（ActBERT, SpatiotemporalMAE, OmniMAE, VideoMAE, MotionMAE）或者不同的训练策略（MaskFeat, VLM,</p><p>ALPRO, All-in-One transformer, MaskViT, CLIP-ViP,Singularity, LF-VILA, EMCL, HiTeA, CHAMPAGNE）</p></li></ul></li><li><p>大语言模型 for 视频理解</p><ul><li>Visual-ChatGPT 使用 LLM 调用视觉模型 API 解决问题</li><li>指令微调的出现进一步增强了这些模型有效相应请求的能力</li></ul></li></ol><p><strong>B.相关综述（动机）</strong></p><p>现有综述的局限性：</p><ul><li>专注于特定子任务：一些论文仅研究视频理解领域的特定子任务，而不是全面概述</li><li>关注范围更广的方法：一些论文关注的方法超出了视频理解的范畴</li></ul><p>尽管现有的调查论文对学术界有重要价值，但它们在基于大型语言模型的一般视频理解任务调查方面留下了空白，这篇论文旨在填补这一空白，通过全面调查使用大型语言模型进行视频理解的任务来弥补现有研究的不足。</p><p>C.综述结构</p><ol type="1"><li>Section II:提供初步认识 for video understanding with LLMs，包括不同粒度级别的各种视频理解任务、相关数据集和评估指标，还有 LLMs 的背景知识</li><li>Section III:深挖最近利用 LLMs 做视频理解的研究的细节，简述方法和影响，分为摘要说的三大类和五小类，还讲解了 Vid-LLMs 的训练策略</li><li>Section IV:提供更多评估 Vid-LLMs 的流行方法的更多信息，以及一些常用基准指标</li><li>Section V:探索 Vid-LLMs 的跨领域的重要应用</li><li>Section VI:总结主要发现并确定未解决的挑战和未来研究的潜在领域</li></ol><h1 id="preliminaries">PRELIMINARIES</h1><p><img src="https://testingcf.jsdelivr.net/gh/nanzhi84/blog-images/202409180058358.png" alt="image-20240914113524520"></p><p>A.视频理解任务</p><ol type="1"><li>Abstract Understanding Tasks<ul><li>Video Classification &amp; Action Recognition（用 class labels 或者 activities labels 分类视频）: Top-K accuracy</li><li>Text-Video Retrieval：Recall at K (<a href="mailto:R@K" class="email">R@K</a>), which measures the accuracy of the first K retrieved results</li><li>Video-to-Text Summarization： Metrics of BLEU, METEOR, CIDEr, and ROUGE-L often evaluate this task</li><li>Video Captioning：BLEU, METEOR, CIDEr, and ROUGE-L</li><li>Video QA： Top-1, Top-K accuracy</li></ul></li><li>Temporal Understanding Tasks<ul><li>Video Summarization（将长视频压缩为短视频）：F1-score, Spearman, and Kendall usually evaluate this task as metrics</li><li>Video Highlight Detection（旨在识别和提取视频中最重要和有趣的片段）</li><li>Temporal Action/Event Localization（识别视频中动作或事件的精确时间片段）</li><li>Temporal Action Proposal Generation（生成候选片段 within 视频中可能包含的动作或活动）</li><li>Video Temporal Grounding（定位视频中与给定文本查询相对应的时刻或间隔）：<a href="mailto:R1@0.5" class="email">R1@0.5</a> and <a href="mailto:R1@0.7" class="email">R1@0.7</a></li><li>Moment Retrieval（识别和提取与给定文本或视觉查询对应的精确视频片段）</li><li>Generic Event Boundary Detection（识别视频中发生重大变化或者过渡的帧，根据不同的事件或活动分割视频）</li><li>Generic Event Boundary Captioning &amp; Grounding（识别和描述视频中重要事件之间的过渡点）</li><li>Dense Video Captioning（为整个视频中发生的多个事件和动作生成详细且连续的文本描述）</li></ul></li><li>Spatiotemporal Understanding Tasks<ul><li>Object Tracking（对象跟踪，持续识别特定对象的轨迹）</li><li>Re-Identification(ReID)（跨不同视频帧或摄像机识别或匹配对象）</li><li>Video Saliency Detection（识别视频中视觉上最重要且最引人注目的区域）</li><li>Video Object Segmentation（将视频划分为与各个对象相对应的片段，随时间描绘它们的边界）</li><li>Video Instance Segmentation（识别、分隔和跟踪视频中每个唯一对象）</li><li>Video Object Referring Segmentation（涉及基于自然语言描述来分割视频中的特定对象）</li><li>Spatiotemporal Grounding（根据给定的查询识别和定位视频空间和时间维度的特定对象或事件）</li></ul></li></ol><p>B.LLMs 的背景</p><p>两个特点</p><ol type="1"><li>Scaling Laws</li><li>Emergent Abilities</li></ol><p>多模态大模型 Multimodal Large Language Models (MLLMs)构成：</p><ul><li>multimodal encoders</li><li>cross-modality aligners</li><li>an LLM core structure</li></ul><h1 id="vid-llms">VID-LLMS</h1><p><img src="https://testingcf.jsdelivr.net/gh/nanzhi84/blog-images/202409180058567.png" alt="image-20240914113612625"></p><p>A. Taxonomy</p><ol type="1"><li>Video Analyzer × LLM：视频分析器被定义为一个模块，接收视频输入并输出视频分析（一般以文本形式）。文本中可能包括视频字幕、密集视频字幕（带有时间戳的视频中所有事件的详细描述）、对象跟踪结果（对象的标签、ID 和边界框）以及视频中存在的其他模式的转录（ASR 语音识别结果或 OCR 的字幕识别结果）。视频文本可以直接输入到后续的 LLM 中，插入输入 LLM 之前预先准备的模板或者转换为临时数据库以供 LLM 检索<ul><li>LLM as Summarizer：LLMs 的主要功能是总结分析从分析器获得的结果。总结根据 prompts 不同而不同，有高度浓缩的摘要文本和标题或回答特定问题的综合摘要等。在这种系统中，信息通常是单向的</li><li>LLM as Manager：LLMs 的主要功能是协调整个系统的运作。它可以根据用户需求主动生成命令来调用不同的视频分析器，然后在输出结果之前选择自己进一步处理该分析或者与视频分析器进行多轮交互，相比 Summarizer 更加灵活</li></ul></li><li>Video Embedder × LLM：Video Embedder 主要指视觉编码器，比如 ViT 或者 CLIP，用来生成视频嵌入。还有一些嵌入器对视频中其他模式进行编码，例如音频（CLAP），LLM 的分词器不作为 Embedder 看待。与视频分析器不同，视频嵌入器的表征不能被直接使用，需要适配器将这些表征映射到 LLMs 的输入的文本语义空间<ul><li>LLM as Text Decoder：LLM 接收视频嵌入并根据 prompts 或 instructions 将其解码为文本输出，一般这些任务不需要细粒度的理解或者精准的时空定位，仅仅主要关注一般的 QA 或 captioning。</li><li>LLM as Regressor：与文本解码器不同，LLM 作为自回归器可以预测连续值，例如视频中的时间戳定位和对象轨迹的边界框坐标（功能类似于回归任务中的回归器，尽管根本上执行的是分类？）。</li><li>LLM as Hidden Layer：LLM 接收视频嵌入作为输入但不直接输出文本，而是连接到专门设定的特定任务头来执行实际的回归任务，例如视频中的事件定位或对象边界框预测，同时还会保留 LLM 的文本输出能力。</li></ul></li><li>(Analyzer + Embedder) × LLM：此类 Vid-LLMs 比较少，它可以同时获取分析器来获取视频的文本分析，并且使用视频嵌入器将视频编码为嵌入。LLM 接收两种类型的输入以及其他 prompt 或 instruction，并输出相应。子类可以为 Summarizer/Manager/Text Decoder/Regressor/Hidden Layer 中的任意一个</li></ol><p>B.Training Strategies for Vid-LLMs</p><ol type="1"><li>Training-free Vid-LLMs：许多 Vid-LLM 系统建立在极其强大的 LLM 之上，具有强大的零样本、情景学习和思维链的能力。大多数 Video Analyzer × LLM 类别的 Vid-LLM 无需培训，因为已经将所有视频信息解析为了文本。因为 LLM 可以将几乎所有 NLP 任务统一为生成任务，因此还可以处理许多其他视频理解任务。代表：SlowFast-LLaVA。</li><li>Fine-tuning Vid-LLMs：与 Video Analyzer × LLM 不同，所有 Video Embedder × LLM 类别中的 Vid-LLM 几乎都需要微调。常用微调方法有：LLM Fully Fine-tuning，Connective Adapter Fine-tuning，Insertive Adapter Fine-tuning，and Fine-tuning with Hybrid Adapters（an adapter is a small，trainable module，用于减少参数训练量）<ul><li>LLM Fully Fine-tuning：一般优于 adapter 微调版本，但是计算资源消耗量大。</li><li>Connective Adapter Fine-tuning：Connective Adapter 一般指的是适配器，即将视频嵌入映射到文本语义空间的 Adapter。在训练空间中，冻结 LLM 和 Embedder，仅仅微调适配器。</li><li>Insertive Adapter Fine-tuning：Insertive Adapter 基于 LoRA，是插入在 LLM 本身的小 Adapter 单元，会影响 LLM 的行为，但是原本的 LLM 和 Embedder 会被冻结。一般存在于 Video Embedder × LLM as Regressor and Video Embedder × LLM as Hidden Layer 两类中，因为这两类需要改变 LLM 的行为去预测连续值。</li><li>Fine-tuning with Hybrid Adapters：使用 Connective Adapter 和 Insertive Adapter 两种组合来实现模态对齐和 LLM 固有行为改变。常见方法是在第一阶段仅微调 Connective Adapter 来实现模态对齐，第二阶段冻结 Connective Adapter，改变训练任务（对齐任务到目标任务）和训练数据（对齐数据到目标数据），然后微调 Insertive Adapter。还有一些同时更新两种 Adapter 的单阶段方法。</li></ul></li></ol><h1 id="benchmarks-and-evaluation">BENCHMARKS AND EVALUATION</h1><p>A.Closed-set Evaluation</p><p>Closed-set evaluations 的基础是带有预定答案的问题。对于 QA 任务，问题被设计为多选题，然后评估正确率。对于 captioning 或者 summarization 任务，the ground truth 是被提前定义的。The CIDEr, METEOR,ROUGE and SPICE metrics are computed by comparing the predicted sentences with the ground-truth sentences。</p><p><img src="https://testingcf.jsdelivr.net/gh/nanzhi84/blog-images/202409180058696.png" alt="image-20240914113629035"></p><p>B.Open-set Evaluation</p><p>与 Closed-set Evaluation 不同，Open-set Evaluation 不依赖于预定义的选项。它通过将 GPT-3.5/4 等模型的预测与自己的答案比较，为预测分配分数。</p><p><img src="https://testingcf.jsdelivr.net/gh/nanzhi84/blog-images/202409180058752.png" alt="image-20240914113639017"></p><p>C.Others</p><p>需要细粒度的事件和空间的视频理解任务，比如 dense captioning, video temporal grounding, spatiotemporal grounding, object tracking, video saliency detection,等需要事件或者时空 annotations 来评估性能，一般用 IoU， <a href="mailto:Recall@K" class="email">Recall@K</a> 和 mAP。有时候用人工评估，不过太费时费力费钱。</p><h1 id="applications">APPLICATIONS</h1><p>A.Media and Entertainment</p><ul><li>Online Video Platforms and Multimedia Information Retrieval</li><li>Video Summarization and Editing</li></ul><p>B.Interactive and User-Centric Technologies</p><ul><li>Virtual Education, Accessibility, and Sign Language</li><li>Interactive Gaming and Virtual Environments</li><li>State-Aware Human-Computer Interaction and Robot Planning</li></ul><p>C.Healthcare and Security Applications</p><ul><li>Healthcare Innovations（医疗保健创新）</li><li>Security, Surveillance, and Cybersecurity（安全、监控和网络安全）</li><li>Advancements in Autonomous Vehicles（自动驾驶）</li></ul><h1 id="future-directions-and-conclusion">FUTURE DIRECTIONS AND CONCLUSION</h1><p>A.Limitations and Future Work</p><ul><li>Fine-grained Video Understanding</li><li>Long-form Video Understanding</li><li>Multimodal Video Understanding</li><li>Human Interaction in Video Understanding</li><li>Hallucination（幻觉） in Multimodal LLMs</li></ul><p>B.Conclusion</p><p>主要讲述了视频理解的发展历程，不同的视频理解任务，Vid-LLMs 的分类及其主要训练方法，视频任务的评估，Vid-LLMs 促进的应用及其不足与未来发展。作者最后认为在以下几个方向可以继续推动进度：</p><ul><li>寻找更有效的训练策略</li><li>提高 Vid-LLMs 的规模</li><li>发展更创新的架构</li><li>扩大数据集规模和增加 benchmarks</li></ul><p>附录</p><p>一.Vid-LLMs 分类表格</p><p><img src="https://testingcf.jsdelivr.net/gh/nanzhi84/blog-images/202409180058823.png" alt="image-20240914113708096"></p><p>二.目前视频大模型的对比表格</p><p><img src="https://testingcf.jsdelivr.net/gh/nanzhi84/blog-images/202409180058891.png" alt="image-20240914113718395"></p></div><div id="comment"><div id="waline-container"></div></div></div><footer id="footer"><div id="footer-wrap"><div>&copy; 2024 - 2024 Orion Blog<span id="footer-icon"><i class="fa-solid fa-font-awesome fa-fw"></i></span> &commat;Orion</div><div>Based on the <a target="_blank" rel="noopener" href="https://hexo.io">Hexo Engine</a> &amp; <a target="_blank" rel="noopener" href="https://github.com/theme-particlex/hexo-theme-particlex">ParticleX Theme</a></div></div></footer></div><transition name="fade"><div id="preview" ref="preview" v-show="previewShow"><img id="preview-content" ref="previewContent"></div></transition></div><script src="/js/main.js"></script><script>Waline.init({el:"#waline-container",serverURL:"https://www.orionverse-comment.blog/",commentCount:!0,pageview:!0,emoji:"https://unpkg.com/@waline/emojis@1.0.1/weibo,https://unpkg.com/@waline/emojis@1.0.1/alus,https://unpkg.com/@waline/emojis@1.0.1/bilibili,https://unpkg.com/@waline/emojis@1.0.1/qq,https://unpkg.com/@waline/emojis@1.0.1/tieba,https://unpkg.com/@waline/emojis@1.0.1/tw-emoji".split(","),meta:"nick,mail,link".split(","),requiredMeta:"nick".split(","),lang:"zh-CN",wordLimit:0,pageSize:"10",login:"enable"})</script></body></html>